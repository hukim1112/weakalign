{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeakAlign demo notebook\n",
    "\n",
    "This notebook shows how to run a trained model on a given image pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "from os.path import exists\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.cnn_geometric_model import CNNGeometric, TwoStageCNNGeometric\n",
    "from data.pf_dataset import PFDataset\n",
    "from data.download_datasets import download_PF_pascal\n",
    "from image.normalization import NormalizeImageDict, normalize_image\n",
    "from util.torch_util import BatchTensorToVars, str_to_bool\n",
    "from geotnf.transformation import GeometricTnf\n",
    "from geotnf.point_tnf import *\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import warnings\n",
    "from torchvision.transforms import Normalize\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.loss import TransformedGridLoss, WeakInlierCount, TwoStageWeakInlierCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the following models:\n",
    "# cnngeo_vgg16, cnngeo_resnet101, proposed_resnet101\n",
    "model_selection = 'proposed_resnet101' \n",
    "\n",
    "model_aff_path = ''\n",
    "model_tps_path = ''\n",
    "model_aff_tps_path = ''\n",
    "\n",
    "if model_selection=='cnngeo_vgg16':\n",
    "    model_aff_path = 'trained_models/trained_models/cnngeo_vgg16_affine.pth.tar'\n",
    "    model_tps_path = 'trained_models/trained_models/cnngeo_vgg16_tps.pth.tar'\n",
    "    feature_extraction_cnn = 'vgg'\n",
    "    \n",
    "elif model_selection=='cnngeo_resnet101':\n",
    "    model_aff_path = 'trained_models/trained_models/cnngeo_resnet101_affine.pth.tar'\n",
    "    model_tps_path = 'trained_models/trained_models/cnngeo_resnet101_tps.pth.tar'   \n",
    "    feature_extraction_cnn = 'resnet101'\n",
    "    \n",
    "elif model_selection=='proposed_resnet101':\n",
    "    model_aff_tps_path = 'trained_models/weakalign_resnet101_affine_tps.pth.tar'\n",
    "    feature_extraction_cnn = 'resnet101'\n",
    "    \n",
    "\n",
    "# source_image_path='datasets/proposal-flow-pascal/PF-dataset-PASCAL/JPEGImages/2008_006325.jpg'\n",
    "# target_image_path='datasets/proposal-flow-pascal/PF-dataset-PASCAL/JPEGImages/2010_004954.jpg'\n",
    "# source_image_path='datasets/1.jpg'\n",
    "# target_image_path='datasets/2.jpg'\n",
    "# source_image_path='datasets/3.JPEG'\n",
    "# target_image_path='datasets/4.JPEG'\n",
    "\n",
    "flowers = ['datasets/1.jpg', 'datasets/2.jpg']\n",
    "dogs = ['datasets/3.JPEG', 'datasets/4.JPEG']\n",
    "armours = ['datasets/5.JPEG', 'datasets/6.JPEG']\n",
    "# if not exists(source_image_path):\n",
    "#     download_PF_pascal('datasets/proposal-flow-pascal/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = dogs[0]\n",
    "target_image_path = flowers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "model = TwoStageCNNGeometric(use_cuda=use_cuda,\n",
    "                             return_correlation=True,\n",
    "                             feature_extraction_cnn=feature_extraction_cnn)\n",
    "\n",
    "# load pre-trained model\n",
    "if model_aff_tps_path!='':\n",
    "    checkpoint = torch.load(model_aff_tps_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "        \n",
    "    for name, param in model.FeatureExtraction.state_dict().items():\n",
    "        model.FeatureExtraction.state_dict()[name].copy_(checkpoint['state_dict']['FeatureExtraction.' + name])    \n",
    "    for name, param in model.FeatureRegression.state_dict().items():\n",
    "        model.FeatureRegression.state_dict()[name].copy_(checkpoint['state_dict']['FeatureRegression.' + name])\n",
    "    for name, param in model.FeatureRegression2.state_dict().items():\n",
    "        model.FeatureRegression2.state_dict()[name].copy_(checkpoint['state_dict']['FeatureRegression2.' + name])    \n",
    "else:\n",
    "    checkpoint_aff = torch.load(model_aff_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint_aff['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint_aff['state_dict'].items()])\n",
    "    for name, param in model.FeatureExtraction.state_dict().items():\n",
    "        model.FeatureExtraction.state_dict()[name].copy_(checkpoint_aff['state_dict']['FeatureExtraction.' + name])    \n",
    "    for name, param in model.FeatureRegression.state_dict().items():\n",
    "        model.FeatureRegression.state_dict()[name].copy_(checkpoint_aff['state_dict']['FeatureRegression.' + name])\n",
    "\n",
    "    checkpoint_tps = torch.load(model_tps_path, map_location=lambda storage, loc: storage)\n",
    "    checkpoint_tps['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint_tps['state_dict'].items()])\n",
    "    for name, param in model.FeatureRegression2.state_dict().items():\n",
    "        model.FeatureRegression2.state_dict()[name].copy_(checkpoint_tps['state_dict']['FeatureRegression.' + name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsTnf = GeometricTnf(geometric_model='tps', use_cuda=use_cuda)\n",
    "affTnf = GeometricTnf(geometric_model='affine', use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizeCNN = GeometricTnf(out_h=240, out_w=240, use_cuda = False) \n",
    "normalizeTnf = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # convert to torch Variable\n",
    "    image = np.expand_dims(image.transpose((2,0,1)),0)\n",
    "    image = torch.Tensor(image.astype(np.float32)/255.0)\n",
    "    image_var = Variable(image,requires_grad=False)\n",
    "\n",
    "    # Resize image using bilinear sampling with identity affine tnf\n",
    "    image_var = resizeCNN(image_var)\n",
    "    \n",
    "    # Normalize image\n",
    "    image_var = normalize_image(image_var)\n",
    "    \n",
    "    return image_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "n_episodes = 100\n",
    "n_way = 20\n",
    "n_shot = 5\n",
    "n_query = 5\n",
    "n_examples = 20\n",
    "im_width, im_height, channels = 28, 28, 1\n",
    "h_dim = 64\n",
    "z_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4112, 20, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load Train Dataset\n",
    "data_generator_path = os.environ['DATA_GENERATOR']\n",
    "train_split_path = os.path.join(data_generator_path, 'labels/omniglot', 'train.txt')\n",
    "with open(train_split_path, 'r') as train_split:\n",
    "    train_classes = [line.rstrip() for line in train_split.readlines()]\n",
    "n_classes = len(train_classes)\n",
    "train_dataset = np.zeros([n_classes, n_examples, im_height, im_width], dtype=np.float32)\n",
    "for i, tc in enumerate(train_classes):\n",
    "    alphabet, character, rotation = tc.split('/')\n",
    "    rotation = float(rotation[3:])\n",
    "    im_dir = os.path.join(data_generator_path, 'datasets/omniglot', alphabet, character)\n",
    "    im_files = sorted(glob.glob(os.path.join(im_dir, '*.png')))\n",
    "    for j, im_file in enumerate(im_files):\n",
    "        im = 1. - np.array(Image.open(im_file).rotate(rotation).resize((im_width, im_height)), np.float32, copy=False)\n",
    "        train_dataset[i, j] = im\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "char1_1 = train_dataset[10][0]\n",
    "char1_2 = train_dataset[10][1]\n",
    "\n",
    "char2_1 = train_dataset[1][0]\n",
    "char2_2 = train_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_img = char1_1\n",
    "target_img = char2_2\n",
    "\n",
    "source_image = np.stack((source_img,)*3, axis=-1)\n",
    "target_image = np.stack((target_img,)*3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa3600ff438>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa35b7d6a20>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAC/tJREFUeJzt3U+MJGUZx/HvI+gFOSwSNxtAVw3xwgFl42lj8KBBYrJ4IXJao8l4kARuEjy4iTEhRjCeTFAIq1HQBJUNMSISFU+GXYKwgPzRLHE3AxuyJsBJgcdD15phmenu6arqqp7n+0k6013T2/Vsdf/mfaveqn4jM5FUz3uGLkDSMAy/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWizl/myiKi1emEV111VVelbNuxY8cGW7eWr+/PWp+fp8yMeZ4XbU7vjYhrgB8A5wE/zszbZjy/VfiHPBU5Yq7tqR2i789an5+n3sMfEecBzwOfBU4CjwE3ZOYzU/6N4ddKqBD+Nvv8nwJezMx/ZuZ/gPuAAy1eT9IStQn/JcC/Njw+2Sx7h4hYi4ijEXG0xbokdaz3A36ZeSdwJ7Tv9kvqTpuW/xRw2YbHlzbLJK2ANuF/DLg8Ij4SEe8DvgQc6aYsSX1buNufmW9GxI3AQ0yG+u7OzKc7q2xkph39dSRg9Qz9DVZj+Dy1Guff9spWeKhvGsO/esb6WYL2n6dlDPVJWmGGXyrK8EtFGX6pKMMvFWX4paKWej3/kDoYPlnod12sW+PT5+dpWWz5paIMv1SU4ZeKMvxSUYZfKsrwS0Wt1FDftOGVWUMnbYdW2qxbGiNbfqkowy8VZfilogy/VJThl4oy/FJRhl8qaqXG+aeZdYll27H4Nv/eS353np1wboctv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8V1WqcPyJOAK8DbwFvZua+Lorqw5Bj6W2/a8DzALo39Fdvj+E97eIkn89k5qsdvI6kJbLbLxXVNvwJ/D4ijkXEWhcFSVqOtt3+/Zl5KiI+CDwcEX/PzEc3PqH5o+AfBmlkoqsLFCLiEPBGZn5vynNW/2qIBeyEg0N6pzG/p5k514sv3O2PiAsi4sKz94HPAccXfT1Jy9Wm278b+HXzF+x84OeZ+btOqpLUu866/XOtrGi3f5YxdyGrWuX3pPduv6TVZvilogy/VJThl4oy/FJRhl8qasd8dfcqa/u149N+7zDg5lZ5KK8rtvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJTj/Nqx2ozl74Rx/Fls+aWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMf5V0Cb6/1XefrvPr9Wfsz/72Wx5ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilomaO80fE3cAXgNOZeUWz7CLgF8Be4ARwfWb+u78yNU2bMeu+zwNwrH685mn57wGuOWfZLcAjmXk58EjzWNIKmRn+zHwUOHPO4gPA4eb+YeC6juuS1LNF9/l3Z+Z6c/9lYHdH9Uhaktbn9mdmRsSWO3YRsQastV2PpG4t2vK/EhF7AJqfp7d6YmbemZn7MnPfguuS1INFw38EONjcPwg80E05kpYl5hjquRe4GrgYeAX4FvAb4JfAh4CXmAz1nXtQcLPX6m/cRwtxqG/nycy5NszM8HfJ8K8ew7t65g2/Z/hJRRl+qSjDLxVl+KWiDL9UlOGXivKru9Urh/PGy5ZfKsrwS0UZfqkowy8VZfilogy/VJThl4pynF+tOI6/umz5paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkox/mL6/t7+zVetvxSUYZfKsrwS0UZfqkowy8VZfilogy/VNTM8EfE3RFxOiKOb1h2KCJORcQTze3afsvUNJm58C0ipt60c83T8t8DXLPJ8u9n5pXN7bfdliWpbzPDn5mPAmeWUIukJWqzz39jRDzZ7Bbs6qwiSUuxaPh/CHwMuBJYB27f6okRsRYRRyPi6ILrktSDmHVhB0BE7AUezMwrtvO7TZ47e2Xatnnew614UG/nycy53tSFWv6I2LPh4ReB41s9V9I4zbykNyLuBa4GLo6Ik8C3gKsj4koggRPA13qsUVIP5ur2d7Yyu/29sNuvjXrt9ktafYZfKsrwS0UZfqkowy8VZfilovzq7hWwzOFY1WHLLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFOc4/An1Okz3rtdueQ+AlwavLll8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXinKcfwmGvB6/7Th8n+cJeI7AsGz5paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqmomeGPiMsi4o8R8UxEPB0RNzXLL4qIhyPihebnrv7LHafMnHqbJSKm3obUZ21tt5vaiTm+SGIPsCczH4+IC4FjwHXAl4EzmXlbRNwC7MrMb8x4rR35jlb+Qow+Q7rK22VImTnXhpvZ8mfmemY+3tx/HXgWuAQ4ABxunnaYyR8ESStiW/v8EbEX+ATwV2B3Zq43v3oZ2N1pZZJ6Nfe5/RHxfuB+4ObMfG1jlywzc6sufUSsAWttC5XUrZn7/AAR8V7gQeChzLyjWfYccHVmrjfHBf6UmR+f8Tru829ilfdt3ecfn872+WPyDtwFPHs2+I0jwMHm/kHgge0WKWk48xzt3w/8BXgKeLtZfCuT/f5fAh8CXgKuz8wzM16rt2ai79bXFq57Y77Uecy1zTJvyz9Xt78rhn+xde9UYw7YmGubpbNuv6SdyfBLRRl+qSjDLxVl+KWiDL9U1Ep9dXefwy9+BfXy9b3dpr2nQ5+VOYZLlm35paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqmolRrnb6PtJZyO5a+eae/ZGMbZh2bLLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFlRnnd1xXy7QKnzdbfqkowy8VZfilogy/VJThl4oy/FJRhl8qamb4I+KyiPhjRDwTEU9HxE3N8kMRcSoinmhu1/ZdbEQMdtPO0vb9zsyptzbrX5aY40ss9gB7MvPxiLgQOAZcB1wPvJGZ35t7ZRHjP/NBov+TdPoMeWbO9eIzz/DLzHVgvbn/ekQ8C1zSrjxJQ9vWPn9E7AU+Afy1WXRjRDwZEXdHxK4t/s1aRByNiKOtKpXUqZnd/v8/MeL9wJ+B72TmryJiN/AqkMC3mewafGXGa9jt10qo0O2fK/wR8V7gQeChzLxjk9/vBR7MzCtmvI7h10qoEP55jvYHcBfw7MbgNwcCz/oicHy7RUoazjxH+/cDfwGeAt5uFt8K3ABcyaTbfwL4WnNwcNpr2fJLPeu0298Vwy/1r7Nuv6SdyfBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1TUsqfofhV4acPji5tlYzTW2sZaF1jborqs7cPzPnGp1/O/a+URRzNz32AFTDHW2sZaF1jbooaqzW6/VJThl4oaOvx3Drz+acZa21jrAmtb1CC1DbrPL2k4Q7f8kgYySPgj4pqIeC4iXoyIW4aoYSsRcSIinmpmHh50irFmGrTTEXF8w7KLIuLhiHih+bnpNGkD1bb0mZu3qG2rmaUH3XZjmvEaBuj2R8R5wPPAZ4GTwGPADZn5zFIL2UJEnAD2ZebgY8IR8WngDeAnZ2dDiojvAmcy87bmD+euzPzGSGo7xDZnbu6ptq1mlv4yA267Lme87sIQLf+ngBcz85+Z+R/gPuDAAHWMXmY+Cpw5Z/EB4HBz/zCTD8/SbVHbKGTmemY+3tx/HTg7s/Sg225KXYMYIvyXAP/a8Pgk45ryO4HfR8SxiFgbuphN7N4wM9LLwO4hi9nEzJmbl+mcmaVHs+0WmfG6ax7we7f9mflJ4PPA15vu7SjlZJ9tTMM1PwQ+xmQat3Xg9iGLaWaWvh+4OTNf2/i7IbfdJnUNst2GCP8p4LINjy9tlo1CZp5qfp4Gfs1kN2VMXjk7SWrz8/TA9fxfZr6SmW9l5tvAjxhw2zUzS98P/Cwzf9UsHnzbbVbXUNttiPA/BlweER+JiPcBXwKODFDHu0TEBc2BGCLiAuBzjG/24SPAweb+QeCBAWt5h7HM3LzVzNIMvO1GN+N1Zi79BlzL5Ij/P4BvDlHDFnV9FPhbc3t66NqAe5l0A//L5NjIV4EPAI8ALwB/AC4aUW0/ZTKb85NMgrZnoNr2M+nSPwk80dyuHXrbTalrkO3mGX5SUR7wk4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9U1P8AcPfX76bxJb0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_image = io.imread(source_image_path)\n",
    "# target_image = io.imread(target_image_path)\n",
    "\n",
    "# source_image_var = preprocess_image(source_image)\n",
    "# target_image_var = preprocess_image(target_image)\n",
    "\n",
    "# if use_cuda:\n",
    "#     source_image_var = source_image_var.cuda()\n",
    "#     target_image_var = target_image_var.cuda()\n",
    "\n",
    "# batch = {'source_image': source_image_var, 'target_image':target_image_var}\n",
    "\n",
    "# resizeTgt = GeometricTnf(out_h=target_image.shape[0], out_w=target_image.shape[1], use_cuda = use_cuda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_var = preprocess_image(source_image)\n",
    "target_image_var = preprocess_image(target_image)\n",
    "\n",
    "if use_cuda:\n",
    "    source_image_var = source_image_var.cuda()\n",
    "    target_image_var = target_image_var.cuda()\n",
    "\n",
    "batch = {'source_image': source_image_var, 'target_image':target_image_var}\n",
    "\n",
    "resizeTgt = GeometricTnf(out_h=target_image.shape[0], out_w=target_image.shape[1], use_cuda = use_cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate model\n",
    "#theta_aff,theta_aff_tps=model(batch)\n",
    "theta_aff,theta_aff_tps,corr_aff,corr_aff_tps=model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_aff : \n",
      " 0.9695 -0.1475  0.0845  0.1470  0.9551  0.1584\n",
      "[torch.cuda.FloatTensor of size 1x6 (GPU 0)]\n",
      ", theta_aff_tps : \n",
      "\n",
      "Columns 0 to 9 \n",
      "-1.0751 -1.1727 -1.0163  0.0118  0.1736 -0.0027  0.9296  0.9355  0.9383 -1.1194\n",
      "\n",
      "Columns 10 to 17 \n",
      "-0.0667  0.7627 -1.2566  0.1773  0.9347 -0.9542 -0.1518  0.9336\n",
      "[torch.cuda.FloatTensor of size 1x18 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"theta_aff : {}, theta_aff_tps : {}\".format(theta_aff.data, theta_aff_tps.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_groups = {'tps_grid_size': 3, 'tps_reg_factor': 0.2, 'normalize_inlier_count': True, 'dilation_filter': 0, 'use_conv_filter': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliersAffine = WeakInlierCount(geometric_model='affine',**arg_groups)\n",
    "#inliersTps = WeakInlierCount(geometric_model='tps',**arg_groups['weak_loss'])\n",
    "inliersComposed = TwoStageWeakInlierCount(use_cuda=use_cuda,**arg_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_comp = inliersComposed(matches=corr_aff,\n",
    "                                                 theta_aff=theta_aff,\n",
    "                                                 theta_aff_tps=theta_aff_tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_aff = inliersAffine(matches=corr_aff,\n",
    "                                theta=theta_aff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute warped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affTpsTnf(source_image, theta_aff, theta_aff_tps, use_cuda=use_cuda):\n",
    "    tpstnf = GeometricTnf(geometric_model = 'tps',use_cuda=use_cuda)\n",
    "    sampling_grid = tpstnf(image_batch=source_image,\n",
    "                           theta_batch=theta_aff_tps,\n",
    "                           return_sampling_grid=True)[1]\n",
    "    X = sampling_grid[:,:,:,0].unsqueeze(3)\n",
    "    Y = sampling_grid[:,:,:,1].unsqueeze(3)\n",
    "    Xp = X*theta_aff[:,0].unsqueeze(1).unsqueeze(2)+Y*theta_aff[:,1].unsqueeze(1).unsqueeze(2)+theta_aff[:,2].unsqueeze(1).unsqueeze(2)\n",
    "    Yp = X*theta_aff[:,3].unsqueeze(1).unsqueeze(2)+Y*theta_aff[:,4].unsqueeze(1).unsqueeze(2)+theta_aff[:,5].unsqueeze(1).unsqueeze(2)\n",
    "    sg = torch.cat((Xp,Yp),3)\n",
    "    warped_image_batch = F.grid_sample(source_image, sg)\n",
    "\n",
    "    return warped_image_batch\n",
    "\n",
    "warped_image_aff = affTnf(batch['source_image'],theta_aff.view(-1,2,3))\n",
    "warped_image_aff_tps = affTpsTnf(batch['source_image'],theta_aff,theta_aff_tps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Un-normalize images and convert to numpy\n",
    "warped_image_aff_np = normalize_image(resizeTgt(warped_image_aff),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "warped_image_aff_tps_np = normalize_image(resizeTgt(warped_image_aff_tps),forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "\n",
    "N_subplots = 4\n",
    "fig, axs = plt.subplots(1,N_subplots)\n",
    "axs[0].imshow(source_image)\n",
    "axs[0].set_title('src')\n",
    "axs[1].imshow(target_image)\n",
    "axs[1].set_title('tgt')\n",
    "axs[2].imshow(warped_image_aff_np)\n",
    "axs[2].set_title('aff')\n",
    "axs[3].imshow(warped_image_aff_tps_np)\n",
    "axs[3].set_title('aff+tps')\n",
    "\n",
    "for i in range(N_subplots):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "fig.set_dpi(150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inliers_aff : \n",
      "1.00000e-02 *\n",
      "  7.7623\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      " inliers_comp : \n",
      "1.00000e-02 *\n",
      "  7.9386\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      " total \n",
      " 0.1570\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ": \n"
     ]
    }
   ],
   "source": [
    "print(\"inliers_aff : {} \\n inliers_comp : {} \\n total {}: \".format(inliers_aff.data, \n",
    "                                                                 inliers_comp.data, \n",
    "                                                                 inliers_aff.data+inliers_comp.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
